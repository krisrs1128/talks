
@article{Arikawa2018TheoreticalFF,
  title={Theoretical framework for analyzing structural compliance properties of proteins},
  author={Keisuke Arikawa},
  journal={Biophysics and Physicobiology},
  year={2018},
  volume={15},
  pages={58 - 74},
  url={https://api.semanticscholar.org/CorpusID:4472297}
}

@BOOK{Hernan2024-ea,
  title     = "Causal inference",
  author    = "Hernan, Miguel A and Robins, James M",
  publisher = "CRC Press",
  month     =  jan,
  year      =  2024,
  address   = "Boca Raton, FL",
  language  = "en"
}

@unpublished{072523_overton,
  title= {Using Synthetic Null Data to Enhance Statistical Rigor in Genomics},
  author = {Jingyi Jessica Li},
  year = {2023}
  note= {ICSB},
  URL= {http://jsb.ucla.edu/sites/default/files/072523_Overton.pdf}
}

@article{Callahan2016BioconductorWF,
  title={Bioconductor Workflow for Microbiome Data Analysis: from raw reads to community analyses},
  author={Benjamin J. Callahan and Kris Sankaran and Julia Fukuyama and Paul J. McMurdie and Susan P. Holmes},
  journal={F1000Research},
  year={2016},
  volume={5},
  url={https://api.semanticscholar.org/CorpusID:6468345}
}

@article{Lhnemann2020ElevenGC,
  title={Eleven grand challenges in single-cell data science},
  author={David Lahnemann and Johannes Koster and Ewa Szczurek and Davis J. McCarthy and Stephanie C. Hicks and Mark D. Robinson and Catalina A. Vallejos and Kieran R. Campbell and Niko Beerenwinkel and Ahmed Mahfouz and Luca Pinello and Pavel Skums and Alexandros Stamatakis and Camille Stephan-Otto Attolini and Samuel Aparicio and Jasmijn A. Baaijens and Marleen Balvert and Buys de Barbanson and Antonio Cappuccio and Giacomo Corleone and Bas E. Dutilh and Maria Florescu and Victor Guryev and Rens Holmer and Katharina Jahn and Thamar Jessurun Lobo and Emma M. Keizer and Indu Khatri and Szymon M. Kiełbasa and Jan O. Korbel and Alexey M. Kozlov and Tzu-Hao Kuo and Boudewijn P. F. Lelieveldt and Ion I. Măndoiu and John C. Marioni and Tobias Marschall and Felix Molder and Amir Niknejad and Lukasz Raczkowski and Marcel J. T. Reinders and Jeroen de Ridder and Antoine-Emmanuel Saliba and Antonios Somarakis and Oliver Stegle and Fabian J Theis and Huan Yang and Alex Zelikovsky and Alice Mchardy and Benjamin J. Raphael and Sohrab P. Shah and Alexander Sch{\"o}nhuth},
  journal={Genome Biology},
  year={2020},
  volume={21},
  url={https://api.semanticscholar.org/CorpusID:211054289}
}

@article{Goodman2001OfPA,
  title={Of P-values and Bayes: a modest proposal.},
  author={Steven N. Goodman},
  journal={Epidemiology},
  year={2001},
  volume={12 3},
  pages={
          295-7
        },
  url={https://api.semanticscholar.org/CorpusID:41986475}
}

@article{Golub1994PerturbationAO,
  title={Perturbation analysis of the canonical correlations of matrix pairs},
  author={Gene H. Golub and Hongyuan Zha},
  journal={Linear Algebra and its Applications},
  year={1994},
  volume={210},
  pages={3-28},
  url={https://api.semanticscholar.org/CorpusID:123160725}
}

@article{Pasolli2017AccessibleCM,
  title={Accessible, curated metagenomic data through ExperimentHub},
  author={Edoardo Pasolli and Lucas Schiffer and Paolo Manghi and Audrey Renson and Valerie Obenchain and Duy Tin Truong and Francesco Beghini and Fa Malik and Marcel Ramos and Jennifer Beam Dowd and Curtis Huttenhower and Martin T. Morgan and N. Segata and Levi Waldron},
  journal={Nature Methods},
  year={2017},
  volume={14},
  pages={1023-1024},
  url={https://api.semanticscholar.org/CorpusID:3403081}
}

@article{Zappia2017SplatterSO,
  title={Splatter: simulation of single-cell RNA sequencing data},
  author={Luke Zappia and Belinda Phipson and Alicia Oshlack},
  journal={Genome Biology},
  year={2017},
  volume={18},
  url={https://api.semanticscholar.org/CorpusID:9332625}
}

@article{Song2023scDesign3GR,
  title={scDesign3 generates realistic in silico data for multimodal single-cell and spatial omics},
  author={Dongyuan Song and Qingyang Wang and Guanao Yan and Tianyang Liu and Tianyi Sun and Jingyi Jessica Li},
  journal={Nature Biotechnology},
  year={2023},
  pages={1-6},
  url={https://api.semanticscholar.org/CorpusID:258638565}
}

@article{Rigby2005GeneralizedAM,
  title={Generalized additive models for location, scale and shape},
  author={Robert A. Rigby and D. M. Stasinopoulos},
  journal={Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  year={2005},
  volume={54},
  url={https://api.semanticscholar.org/CorpusID:56076930}
}

@article{Hofner2014gamboostLSSAR,
  title={gamboostLSS: An R Package for Model Building and Variable Selection in the GAMLSS Framework},
  author={Benjamin Hofner and Andreas Mayr and Matthias Schmid},
  journal={arXiv: Computation},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:62302678}
}

@article{Cao2020ManifoldAF,
  title={Manifold alignment for heterogeneous single-cell multi-omics data integration using Pamona},
  author={Kai Cao and Yiguang Hong and Lin Wan},
  journal={Bioinformatics},
  year={2020},
  volume={38},
  pages={211 - 219},
  url={https://api.semanticscholar.org/CorpusID:226959190}
}

@article{Gosmann2017LactobacillusDeficientCB,
  title={Lactobacillus‐Deficient Cervicovaginal Bacterial Communities Are Associated with Increased HIV Acquisition in Young South African Women},
  author={Christina Gosmann and Melis N. Anahtar and Scott A. Handley and Mara Farcasanu and Galeb S. Abu-Ali and Brittany A. Bowman and Nikita Padavattan and Chandni Desai and Lindsay Droit and Amber Moodley and Mary Dong and Yuezhou Chen and Nasreen Ismail and Thumbi Ndung’u and Musie S. Ghebremichael and Duane R. Wesemann and Caroline M. Mitchell and Krista L. Dong and Curtis Huttenhower and Bruce D. Walker and Herbert W. Virgin and Douglas S. Kwon},
  journal={Immunity},
  year={2017},
  volume={46},
  pages={29–37}
}

@article{Liu2019LatentSC,
  title={Latent Space Cartography: Visual Analysis of Vector Space Embeddings},
  author={Yang Liu and Eunice Jun and Qisheng Li and Jeffrey Heer},
  journal={Computer Graphics Forum},
  year={2019},
  volume={38},
}

@inproceedings{Adebayo2018SanityCF,
  title={Sanity Checks for Saliency Maps},
  author={Julius Adebayo and Justin Gilmer and Michael Muelly and Ian J. Goodfellow and Moritz Hardt and Been Kim},
  booktitle={Neural Information Processing Systems},
  year={2018},
}

@article{Bilodeau2022ImpossibilityTF,
  title={Impossibility Theorems for Feature Attribution},
  author={Blair Bilodeau and Natasha Jaques and Pang Wei Koh and Been Kim},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  year={2022},
  volume={121 2},
  pages={
          e2304406120
        },
}

@article{Koh2020ConceptBM,
  title={Concept Bottleneck Models},
  author={Pang Wei Koh and Thao Nguyen and Yew Siang Tang and Stephen Mussmann and Emma Pierson and Been Kim and Percy Liang},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.04612}
}

@inproceedings{Wu2021PolyjuiceGC,
  title={Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models},
  author={Tongshuang Sherry Wu and Marco Tulio Ribeiro and Jeffrey Heer and Daniel S. Weld},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}

@article{Coenen2019VisualizingAM,
  title={Visualizing and Measuring the Geometry of BERT},
  author={Andy Coenen and Emily Reif and Ann Yuan and Been Kim and Adam Pearce and Fernanda B. Vi{\'e}gas and Martin Wattenberg},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.02715}
}

@article{Bansal2020DoesTW,
  title={Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance},
  author={Gagan Bansal and Tongshuang Sherry Wu and Joyce Zhou and Raymond Fok and Besmira Nushi and Ece Kamar and Marco Tulio Ribeiro and Daniel S. Weld},
  journal={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  year={2020}
}

@article{Lipton2016TheMO,
  title={The mythos of model interpretability},
  author={Zachary Chase Lipton},
  journal={Communications of the ACM},
  year={2016},
  volume={61},
  pages={36 - 43},
}

@article{Rudin2018StopEB,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Cynthia Rudin},
  journal={Nature Machine Intelligence},
  year={2018},
  volume={1},
  pages={206 - 215},
}

@article{Murdoch2019DefinitionsMA,
  title={Definitions, methods, and applications in interpretable machine learning},
  author={W. James Murdoch and Chandan Singh and Karl Kumbier and Reza Abbasi-Asl and Bin Yu},
  journal={Proceedings of the National Academy of Sciences},
  year={2019},
  volume={116},
  pages={22071 - 22080},
}

@article{Caruana2015IntelligibleMF,
  title={Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission},
  author={Rich Caruana and Yin Lou and Johannes Gehrke and Paul Koch and M. Sturm and No{\'e}mie Elhadad},
  journal={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year={2015},
}

@article{Gu2017BadNetsIV,
  title={BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain},
  author={Tianyu Gu and Brendan Dolan-Gavitt and Siddharth Garg},
  journal={ArXiv},
  year={2017},
  volume={abs/1708.06733}
}

@ARTICLE{Hutchins1985-oz,
  title     = "Direct manipulation interfaces",
  author    = "Hutchins, Edwin L and Hollan, James D and Norman, Donald A",
  abstract  = "Direct manipulation has been lauded as a good form of interface
               design, and some interfaces that have this property have been
               well received by users. In this article we seek a cognitive
               account of both the advantages and disadvantages of direct
               manipulation interfaces. We identify two underlying phenomena
               that give rise to the feeling of directness. One deals with the
               information processing distance between the user's intentions
               and the facilities provided by the machine. Reduction of this
               distance makes the interface feel direct by reducing the effort
               required of the user to accomplish goals. The second phenomenon
               concerns the relation between the input and output vocabularies
               of the interface language. In particular, direct manipulation
               requires that the system provide representations of objects that
               behave as if they are the objects themselves. This provides the
               feeling of directness of manipulation.",
  journal   = "Hum.-Comput. Interact.",
  publisher = "Informa UK Limited",
  volume    =  1,
  number    =  4,
  pages     = "311--338",
  month     =  dec,
  year      =  1985,
  language  = "en"
}

@book{munzner2014visualization,
  title={Visualization analysis and design},
  author={Munzner, Tamara},
  year={2014},
  publisher={CRC press}
}

@InProceedings{pmlr-v119-koh20a,
  title = 	 {Concept Bottleneck Models},
  author =       {Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5338--5348},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/koh20a/koh20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/koh20a.html},
  abstract = 	 {We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art models today do not typically support the manipulation of concepts like "the existence of bone spurs", as they are trained end-to-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts ("bone spurs") or bird attributes ("wing color"). These models also allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time.}
}

@article{Bengio2009,
  title = {Learning Deep Architectures for AI},
  volume = {2},
  ISSN = {1935-8245},
  url = {http://dx.doi.org/10.1561/2200000006},
  DOI = {10.1561/2200000006},
  number = {1},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  publisher = {Now Publishers},
  author = {Bengio,  Y.},
  year = {2009},
  pages = {1–127}
}

@misc{
alain2017understanding,
title={Understanding intermediate layers using linear classifier probes},
author={Guillaume Alain and Yoshua Bengio},
year={2017},
url={https://openreview.net/forum?id=ryF7rTqgl}
}

@article{templeton2024scaling,
    title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
    author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
    year={2024},
    journal={Transformer Circuits Thread},
    url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}

@ARTICLE{Friedman2001-xk,
  title     = "Greedy function approximation: A gradient boosting machine",
  author    = "Friedman, Jerome H",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  29,
  number    =  5,
  pages     = "1189--1232",
  month     =  oct,
  year      =  2001
}

@ARTICLE{Kim2017-kb,
  title        = "Interpretability beyond feature attribution: Quantitative
                  testing with Concept Activation Vectors ({TCAV})",
  author       = "Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai,
                  Carrie and Wexler, James and Viegas, Fernanda and Sayres,
                  Rory",
  abstract     = "The interpretation of deep learning models is a challenge due
                  to their size, complexity, and often opaque internal state.
                  In addition, many systems, such as image classifiers, operate
                  on low-level features rather than high-level concepts. To
                  address these challenges, we introduce Concept Activation
                  Vectors (CAVs), which provide an interpretation of a neural
                  net's internal state in terms of human-friendly concepts. The
                  key idea is to view the high-dimensional internal state of a
                  neural net as an aid, not an obstacle. We show how to use
                  CAVs as part of a technique, Testing with CAVs (TCAV), that
                  uses directional derivatives to quantify the degree to which
                  a user-defined concept is important to a classification
                  result--for example, how sensitive a prediction of ``zebra''
                  is to the presence of stripes. Using the domain of image
                  classification as a testing ground, we describe how CAVs may
                  be used to explore hypotheses and generate insights for a
                  standard image classification network as well as a medical
                  application.",
  journal      = "arXiv [stat.ML]",
  publisher    = "arXiv",
  year         =  2017,
  primaryClass = "stat.ML"
}

@INPROCEEDINGS{Yun2021-jc,
  title      = "Transformer visualization via dictionary learning:
                contextualized embedding as a linear superposition of
                transformer factors",
  booktitle  = "Proceedings of Deep Learning Inside Out ({DeeLIO)}: The 2nd
                Workshop on Knowledge Extraction and Integration for Deep
                Learning Architectures",
  author     = "Yun, Zeyu and Chen, Yubei and Olshausen, Bruno and LeCun, Yann",
  publisher  = "Association for Computational Linguistics",
  year       =  2021,
  address    = "Stroudsburg, PA, USA",
  conference = "Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd
                Workshop on Knowledge Extraction and Integration for Deep
                Learning Architectures",
  location   = "Online"
}

@ARTICLE{Apley2020-nd,
  title     = "Visualizing the effects of predictor variables in black box
               supervised learning models",
  author    = "Apley, Daniel W and Zhu, Jingyu",
  abstract  = "Summary In many supervised learning applications, understanding
               and visualizing the effects of the predictor variables on the
               predicted response is of paramount importance. A shortcoming of
               black box supervised learning models (e.g. complex trees, neural
               networks, boosted trees, random forests, nearest neighbours,
               local kernel-weighted methods and support vector regression) in
               this regard is their lack of interpretability or transparency.
               Partial dependence plots, which are the most popular approach
               for visualizing the effects of the predictors with black box
               supervised learning models, can produce erroneous results if the
               predictors are strongly correlated, because they require
               extrapolation of the response at predictor values that are far
               outside the multivariate envelope of the training data. As an
               alternative to partial dependence plots, we present a new
               visualization approach that we term accumulated local effects
               plots, which do not require this unreliable extrapolation with
               correlated predictors. Moreover, accumulated local effects plots
               are far less computationally expensive than partial dependence
               plots. We also provide an R package ALEPlot as supplementary
               material to implement our proposed method.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Oxford University Press (OUP)",
  volume    =  82,
  number    =  4,
  pages     = "1059--1086",
  month     =  sep,
  year      =  2020,
  copyright = "https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model",
  language  = "en"
}

@ARTICLE{Agarwal2023-sx,
  title        = "{MDI+}: A flexible random forest-based feature importance
                  framework",
  author       = "Agarwal, Abhineet and Kenney, Ana M and Tan, Yan Shuo and
                  Tang, Tiffany M and Yu, Bin",
  abstract     = "Mean decrease in impurity (MDI) is a popular feature
                  importance measure for random forests (RFs). We show that the
                  MDI for a feature $X_k$ in each tree in an RF is equivalent
                  to the unnormalized $R^2$ value in a linear regression of the
                  response on the collection of decision stumps that split on
                  $X_k$. We use this interpretation to propose a flexible
                  feature importance framework called MDI+. Specifically, MDI+
                  generalizes MDI by allowing the analyst to replace the linear
                  regression model and $R^2$ metric with regularized
                  generalized linear models (GLMs) and metrics better suited
                  for the given data structure. Moreover, MDI+ incorporates
                  additional features to mitigate known biases of decision
                  trees against additive or smooth models. We further provide
                  guidance on how practitioners can choose an appropriate GLM
                  and metric based upon the Predictability, Computability,
                  Stability framework for veridical data science. Extensive
                  data-inspired simulations show that MDI+ significantly
                  outperforms popular feature importance measures in
                  identifying signal features. We also apply MDI+ to two
                  real-world case studies on drug response prediction and
                  breast cancer subtype classification. We show that MDI+
                  extracts well-established predictive genes with significantly
                  greater stability compared to existing feature importance
                  measures. All code and models are released in a full-fledged
                  python package on Github.",
  year         =  2023,
  primaryClass = "stat.ME",
  eprint       = "2307.01932"
}

@INPROCEEDINGS{Ribeiro2016-qk,
  title           = "Why should {I} trust you?",
  booktitle       = "Proceedings of the 22nd {ACM} {SIGKDD} International
                     Conference on Knowledge Discovery and Data Mining",
  author          = "Ribeiro, Marco Tulio and Singh, Sameer and Guestrin,
                     Carlos",
  publisher       = "ACM",
  month           =  aug,
  year            =  2016,
  address         = "New York, NY, USA",
  copyright       = "http://www.acm.org/publications/policies/copyright\_policy\#Background",
  conference      = "KDD '16: The 22nd ACM SIGKDD International Conference on
                     Knowledge Discovery and Data Mining",
  location        = "San Francisco California USA"
}

@misc{stoehr2024,
  doi = {10.48550/ARXIV.2403.19851},
  url = {https://arxiv.org/abs/2403.19851},
  author = {Stoehr,  Niklas and Gordon,  Mitchell and Zhang,  Chiyuan and Lewis,  Owen},
  keywords = {Computation and Language (cs.CL),  Cryptography and Security (cs.CR),  Machine Learning (cs.LG),  Machine Learning (stat.ML),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Localizing Paragraph Memorization in Language Models},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{karpathy2015,
  doi = {10.48550/ARXIV.1506.02078},
  url = {https://arxiv.org/abs/1506.02078},
  author = {Karpathy,  Andrej and Johnson,  Justin and Fei-Fei,  Li},
  keywords = {Machine Learning (cs.LG),  Computation and Language (cs.CL),  Neural and Evolutionary Computing (cs.NE),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Visualizing and Understanding Recurrent Networks},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@article{Lucieri2022,
  title = {ExAID: A multimodal explanation framework for computer-aided diagnosis of skin lesions},
  volume = {215},
  ISSN = {0169-2607},
  url = {http://dx.doi.org/10.1016/j.cmpb.2022.106620},
  DOI = {10.1016/j.cmpb.2022.106620},
  journal = {Computer Methods and Programs in Biomedicine},
  publisher = {Elsevier BV},
  author = {Lucieri,  Adriano and Bajwa,  Muhammad Naseer and Braun,  Stephan Alexander and Malik,  Muhammad Imran and Dengel,  Andreas and Ahmed,  Sheraz},
  year = {2022},
  month = mar,
  pages = {106620}
}

@inproceedings{atanasova-etal-2020-diagnostic,
    title = "A Diagnostic Study of Explainability Techniques for Text Classification",
    author = "Atanasova, Pepa  and
      Simonsen, Jakob Grue  and
      Lioma, Christina  and
      Augenstein, Isabelle",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.263",
    doi = "10.18653/v1/2020.emnlp-main.263",
    pages = "3256--3274",
    abstract = "Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models{'} predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model{'}s performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.",
}

@Manual{lime_package,
  title = {lime: Local Interpretable Model-Agnostic Explanations},
  author = {Emil Hvitfeldt and Thomas Lin Pedersen and Michaël Benesty},
  year = {2022},
  note = {https://lime.data-imaginist.com, https://github.com/thomasp85/lime},
}

% Other papers to include
% the recent concept papers - are they reliable?
% some of the evaluation papers?
% Some packages. You can have a pointer to relevant packages at the very end
% your own paper!!
% 

@inproceedings{
freeman2024exploring,
title={Exploring Memorization and Copyright Violation in Frontier {LLM}s: A Study of the New York Times v. Open{AI} 2023 Lawsuit},
author={Joshua Freeman and Chloe Rippe and Edoardo Debenedetti and Maksym Andriushchenko},
booktitle={Neurips Safe Generative AI Workshop 2024},
year={2024},
url={https://openreview.net/forum?id=C66DBl9At8}
}

@misc{nyt_case,
  title        = {The New York Times Company Lawsuit Document 1-68},
  number       = {1:23-cv-11195},
  court        = {U.S. District Court},
  year         = {2023},
  type         = {Court Filing},
  note         = {Case No. 1:23-cv-11195, Document 1-68}
}

@inbook{Zeiler2014,
  title = {Visualizing and Understanding Convolutional Networks},
  ISBN = {9783319105901},
  ISSN = {1611-3349},
  url = {http://dx.doi.org/10.1007/978-3-319-10590-1_53},
  DOI = {10.1007/978-3-319-10590-1_53},
  booktitle = {Computer Vision – ECCV 2014},
  publisher = {Springer International Publishing},
  author = {Zeiler,  Matthew D. and Fergus,  Rob},
  year = {2014},
  pages = {818–833}
}

@article{Caruana2015IntelligibleMF,
  title={Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission},
  author={Rich Caruana and Yin Lou and Johannes Gehrke and Paul Koch and M. Sturm and No{\'e}mie Elhadad},
  journal={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year={2015},
}


@article{kundaliya2023,
author={Kundaliya,Dev},
year={2023},
month={Feb 09},
title={Computing - Incisive Media: Google AI chatbot Bard gives wrong answer in its first demo},
journal={Computing},
keywords={Computers; Accuracy; Solar system; Space telescopes; Artificial intelligence; Chatbots},
isbn={13612972},
language={English},
url={https://ezproxy.library.wisc.edu/login?url=https://www.proquest.com/trade-journals/computing-incisive-media-google-ai-chatbot-bard/docview/2774438186/se-2},
}

@ARTICLE{Sankaran2024-ny,
  title     = "Data Science Principles for Interpretable and Explainable {AI}",
  author    = "Sankaran, Kris",
  abstract  = "Society's capacity for algorithmic problem-solving has never
               been greater. Artificial Intelligence is now applied across more
               domains than ever, a consequence of powerful abstractions,
               abundant data, and accessible software. As capabilities have
               expanded, so have risks, with models often deployed without
               fully understanding their potential impacts. Interpretable and
               interactive machine learning aims to make complex models more
               transparent and controllable, enhancing user agency. This review
               synthesizes key principles from the growing literature in this
               field. We first introduce precise vocabulary for discussing
               interpretability, like the distinction between glass box and
               explainable algorithms. We then explore connections to classical
               statistical and design principles, like parsimony and the gulfs
               of interaction. Basic explainability techniques -- including
               learned embeddings, integrated gradients, and concept
               bottlenecks -- are illustrated with a simple case study. We also
               review criteria for objectively evaluating interpretability
               approaches. Throughout, we underscore the importance of
               considering audience goals when designing interactive
               algorithmic systems. Finally, we outline open challenges and
               discuss the potential role of data science in addressing them.
               Code to reproduce all examples can be found at
               https://go.wisc.edu/3k1ewe.",
  journal = "arXiv",
  year      =  2024
}

@article{kundaliya2023,
author={Kundaliya,Dev},
year={2023},
month={Feb 09},
title={Computing - Incisive Media: Google AI chatbot Bard gives wrong answer in its first demo},
journal={Computing},
keywords={Computers; Accuracy; Solar system; Space telescopes; Artificial intelligence; Chatbots},
isbn={13612972},
language={English},
url={https://ezproxy.library.wisc.edu/login?url=https://www.proquest.com/trade-journals/computing-incisive-media-google-ai-chatbot-bard/docview/2774438186/se-2},
}

@article{Gu2019,
  title = {BadNets: Evaluating Backdooring Attacks on Deep Neural Networks},
  volume = {7},
  ISSN = {2169-3536},
  url = {http://dx.doi.org/10.1109/ACCESS.2019.2909068},
  DOI = {10.1109/access.2019.2909068},
  journal = {IEEE Access},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author = {Gu,  Tianyu and Liu,  Kang and Dolan-Gavitt,  Brendan and Garg,  Siddharth},
  year = {2019},
  pages = {47230–47244}
}

@inproceedings{50351,title	= {Concept Bottleneck Models},author	= {Pang Wei Koh and Thao Nguyen and Yew Siang Tang and Stephen Mussmann and Emma Pierson and Been Kim and Percy Liang},year	= {2020}}


@inproceedings{
yuksekgonul2023posthoc,
title={Post-hoc Concept Bottleneck Models},
author={Mert Yuksekgonul and Maggie Wang and James Zou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=nA5AZ8CEyow}
}

@inbook{10.5555/3454287.3455119,
author = {Ghorbani, Amirata and Wexler, James and Zou, James and Kim, Been},
title = {Towards automatic concept-based explanations},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for concept based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that ACE discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {832},
numpages = {10}
}

@misc{overlooked_factors,
  doi = {10.48550/ARXIV.2207.09615},
  url = {https://arxiv.org/abs/2207.09615},
  author = {Ramaswamy,  Vikram V. and Kim,  Sunnie S. Y. and Fong,  Ruth and Russakovsky,  Olga},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Overlooked factors in concept-based explanations: Dataset choice,  concept learnability,  and human capability},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@misc{medium_beyond_nodate,
	title = {Beyond {Interpretability}: {Developing} a {Language} to {Shape} {Our} {Relationships} with {AI}},
	shorttitle = {Beyond {Interpretability}},
	url = {https://cacmb4.acm.org/opinion/articles/260816-beyond-interpretability-developing-a-language-to-shape-our-relationships-with-ai/fulltext},
	abstract = {Wouldn\&\#39;t it be nice if we could ask AI questions to learn how and why it makes its predictions?},
	language = {en},
  year={2022},
	urldate = {2024-05-16},
	author = {Been Kim},
  journal = {Communications of the ACM},
	file = {Snapshot:/Users/krissankaran/Zotero/storage/WT4FPMM7/fulltext.html:text/html},
}

@InProceedings{tcav,
  title = 	 {Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors ({TCAV})},
  author =       {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and sayres, Rory},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2668--2677},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kim18d/kim18d.pdf},
  url = 	 {https://proceedings.mlr.press/v80/kim18d.html},
  abstract = 	 {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result–for example, how sensitive a prediction of “zebra” is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.}
}

@article{chen2018shapley,
  title={L-shapley and c-shapley: Efficient model interpretation for structured data},
  author={Chen, Jianbo and Song, Le and Wainwright, Martin J and Jordan, Michael I},
  journal={arXiv preprint arXiv:1808.02610},
  year={2018}
}
