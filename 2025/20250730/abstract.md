Our society’s capacity for algorithmic problem solving has perhaps never been higher — the combination of powerful abstractions, plentiful data, and accessible software means that machine learning is now a central tool across various industries and areas of research. At the same time, we are regularly warned of the risks of misusing this capacity (e.g., the New York Times in 2024: “Imran Khan’s ‘Victory Speech’ From Jail Shows A.I.’s Peril and Promise,” “The Trouble With A.I. Sharpening,” “Are A.I. Mammograms Worth Their Cost?”). Interpretable machine learning aims to make complex models understandable and controllable, enhancing user agency and creativity. This tutorial will review principles from the growing literature on this topic. We will introduce precise vocabulary for discussing interpretability, for example, the distinction between glassbox and explainable algorithms. Connections to classical statistical and design principles, like parsimony and the gulfs of interaction, will be highlighted. We will review the mechanics of basic explainability techniques, including SHAP values and SAE visualization. Real-world applications and simplified code demos will illustrate how interpretability can support progress on longstanding challenges, like out-of-domain generalization and safety. Criteria for objectively evaluating interpretability mechanisms will be discussed. Finally, we will explore open problems and the role for statistical thinking in addressing them. This tutorial is a summary of a seminar taught at UW-Madison in Spring 2025: https://go.wisc.edu/4kddhw