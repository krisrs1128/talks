@article{Leek2010,
  title = {Tackling the widespread and critical impact of batch effects in high-throughput data},
  volume = {11},
  ISSN = {1471-0064},
  url = {http://dx.doi.org/10.1038/nrg2825},
  DOI = {10.1038/nrg2825},
  number = {10},
  journal = {Nature Reviews Genetics},
  publisher = {Springer Science and Business Media LLC},
  author = {Leek,  Jeffrey T. and Scharpf,  Robert B. and Bravo,  Héctor Corrada and Simcha,  David and Langmead,  Benjamin and Johnson,  W. Evan and Geman,  Donald and Baggerly,  Keith and Irizarry,  Rafael A.},
  year = {2010},
  month = sep,
  pages = {733–739}
}

@article{Wang2019,
  title = {Managing batch effects in microbiome data},
  volume = {21},
  ISSN = {1477-4054},
  url = {http://dx.doi.org/10.1093/bib/bbz105},
  DOI = {10.1093/bib/bbz105},
  number = {6},
  journal = {Briefings in Bioinformatics},
  publisher = {Oxford University Press (OUP)},
  author = {Wang,  Yiwen and L\^eCao,  Kim-Anh},
  year = {2019},
  month = nov,
  pages = {1954–1970}
}

@article{Johnson2006,
  title = {Adjusting batch effects in microarray expression data using empirical Bayes methods},
  volume = {8},
  ISSN = {1465-4644},
  url = {http://dx.doi.org/10.1093/biostatistics/kxj037},
  DOI = {10.1093/biostatistics/kxj037},
  number = {1},
  journal = {Biostatistics},
  publisher = {Oxford University Press (OUP)},
  author = {Johnson,  W. Evan and Li,  Cheng and Rabinovic,  Ariel},
  year = {2006},
  month = apr,
  pages = {118–127}
}

@article{Korsunsky2019,
  title = {Fast,  sensitive and accurate integration of single-cell data with Harmony},
  volume = {16},
  ISSN = {1548-7105},
  url = {http://dx.doi.org/10.1038/s41592-019-0619-0},
  DOI = {10.1038/s41592-019-0619-0},
  number = {12},
  journal = {Nature Methods},
  publisher = {Springer Science and Business Media LLC},
  author = {Korsunsky,  Ilya and Millard,  Nghia and Fan,  Jean and Slowikowski,  Kamil and Zhang,  Fan and Wei,  Kevin and Baglaenko,  Yuriy and Brenner,  Michael and Loh,  Po-ru and Raychaudhuri,  Soumya},
  year = {2019},
  month = nov,
  pages = {1289–1296}
}

@misc{githubGitHubGtonkinhillTCGA_analysis,
	author = {Gerry Tonkin-Hill},
	title = {{G}it{H}ub - gtonkinhill/{T}{C}{G}{A}\_analysis --- github.com},
	howpublished = {\url{https://github.com/gtonkinhill/TCGA_analysis}},
	year = {2023},
	note = {[Accessed 21-06-2024]},
}
@article{Weiss2017,
  title = {Normalization and microbial differential abundance strategies depend upon data characteristics},
  volume = {5},
  ISSN = {2049-2618},
  url = {http://dx.doi.org/10.1186/s40168-017-0237-y},
  DOI = {10.1186/s40168-017-0237-y},
  number = {1},
  journal = {Microbiome},
  publisher = {Springer Science and Business Media LLC},
  author = {Weiss,  Sophie and Xu,  Zhenjiang Zech and Peddada,  Shyamal and Amir,  Amnon and Bittinger,  Kyle and Gonzalez,  Antonio and Lozupone,  Catherine and Zaneveld,  Jesse R. and Vázquez-Baeza,  Yoshiki and Birmingham,  Amanda and Hyde,  Embriette R. and Knight,  Rob},
  year = {2017},
  month = mar 
}

@article{Ma2024,
  title = {Principled and interpretable alignability testing and integration of single-cell data},
  volume = {121},
  ISSN = {1091-6490},
  url = {http://dx.doi.org/10.1073/pnas.2313719121},
  DOI = {10.1073/pnas.2313719121},
  number = {10},
  journal = {Proceedings of the National Academy of Sciences},
  publisher = {Proceedings of the National Academy of Sciences},
  author = {Ma,  Rong and Sun,  Eric D. and Donoho,  David and Zou,  James},
  year = {2024},
  month = feb 
}

@article{Nygaard2015,
  title = {Methods that remove batch effects while retaining group differences may lead to exaggerated confidence in downstream analyses},
  volume = {17},
  ISSN = {1465-4644},
  url = {http://dx.doi.org/10.1093/biostatistics/kxv027},
  DOI = {10.1093/biostatistics/kxv027},
  number = {1},
  journal = {Biostatistics},
  publisher = {Oxford University Press (OUP)},
  author = {Nygaard,  Vegard and Rødland,  Einar Andreas and Hovig,  Eivind},
  year = {2015},
  month = aug,
  pages = {29–39}
}

@article{Soneson2014,
  title = {Batch Effect Confounding Leads to Strong Bias in Performance Estimates Obtained by Cross-Validation},
  volume = {9},
  ISSN = {1932-6203},
  url = {http://dx.doi.org/10.1371/journal.pone.0100335},
  DOI = {10.1371/journal.pone.0100335},
  number = {6},
  journal = {PLoS ONE},
  publisher = {Public Library of Science (PLoS)},
  author = {Soneson,  Charlotte and Gerster,  Sarah and Delorenzi,  Mauro},
  editor = {Zhang,  Shu-Dong},
  year = {2014},
  month = jun,
  pages = {e100335}
}

@article{Goodsell2018,
  title = {From Atoms to Cells: Using Mesoscale Landscapes to Construct Visual Narratives},
  volume = {430},
  ISSN = {0022-2836},
  url = {http://dx.doi.org/10.1016/j.jmb.2018.06.009},
  DOI = {10.1016/j.jmb.2018.06.009},
  number = {21},
  journal = {Journal of Molecular Biology},
  publisher = {Elsevier BV},
  author = {Goodsell,  David S. and Franzen,  Margaret A. and Herman,  Tim},
  year = {2018},
  month = oct,
  pages = {3954–3968}
}

@misc{githubGitHubGtonkinhillTCGA_analysis,
	author = {Gerry Tonkin-Hill},
	title = {{G}it{H}ub - gtonkinhill/{T}{C}{G}{A}\_analysis --- github.com},
	howpublished = {\url{https://github.com/gtonkinhill/TCGA_analysis}},
	year = {2023},
	note = {[Accessed 21-06-2024]},
}

@article{Sankaran2024,
  title = {Semisynthetic Simulation for Microbiome Data Analysis},
  url = {http://dx.doi.org/10.1101/2024.10.14.618211},
  DOI = {10.1101/2024.10.14.618211},
  publisher = {Cold Spring Harbor Laboratory},
  author = {Sankaran,  Kris and Kodikara,  Saritha and Li,  Jingyi Jessica and Cao,  Kim-Anh L\^e},
  year = {2024},
  month = oct 
}

@article{retraction2024,
  title= {Journal retracts influential cancer microbiome paper},
  author = {Catherine Offord},
  url = {http://dx.doi.org/10.1126/science.z2xkuua},
  DOI = {10.1126/science.z2xkuua},
  journal = {AAAS Articles DO Group},
  publisher = {American Association for the Advancement of Science (AAAS)},
  year = {2024},
  month = jun 
}

@article{Poore2020,
  title = {RETRACTED ARTICLE: Microbiome analyses of blood and tissues suggest cancer
diagnostic approach},
  volume = {579},
  ISSN = {1476-4687},
  url = {http://dx.doi.org/10.1038/s41586-020-2095-1},
  DOI = {10.1038/s41586-020-2095-1},
  number = {7800},
  journal = {Nature},
  publisher = {Springer Science and Business Media LLC},
  author = {Poore,  Gregory D. and Kopylova,  Evguenia and Zhu,  Qiyun and Carpenter,  Carolina and Fraraccio,  Serena and Wandro,  Stephen and Kosciolek,  Tomasz and Janssen,  Stefan and Metcalf,  Jessica and Song,  Se Jin and Kanbar,  Jad and Miller-Montgomery,  Sandrine and Heaton,  Robert and Mckay,  Rana and Patel,  Sandip Pravin and Swafford,  Austin D. and Knight,  Rob},
  year = {2020},
  month = mar,
  pages = {567–574}
}

@article{Gerard2021,
  title = {Unifying and Generalizing Methods for Removing Unwanted Variation Based on Negative Controls},
  ISSN = {1017-0405},
  url = {http://dx.doi.org/10.5705/ss.202018.0345},
  DOI = {10.5705/ss.202018.0345},
  journal = {Statistica Sinica},
  publisher = {Statistica Sinica (Institute of Statistical Science)},
  author = {Gerard,  David and Stephens,  Matthew},
  year = {2021}
}

@article{GagnonBartsch2011,
  title = {Using control genes to correct for unwanted variation in microarray data},
  volume = {13},
  ISSN = {1468-4357},
  url = {http://dx.doi.org/10.1093/biostatistics/kxr034},
  DOI = {10.1093/biostatistics/kxr034},
  number = {3},
  journal = {Biostatistics},
  publisher = {Oxford University Press (OUP)},
  author = {Gagnon-Bartsch,  J. A. and Speed,  T. P.},
  year = {2011},
  month = nov,
  pages = {539–552}
}

@article{Jacob2015,
  title = {Correcting gene expression data when neither the unwanted variation nor the
          factor of interest are observed},
  volume = {17},
  ISSN = {1468-4357},
  url = {http://dx.doi.org/10.1093/biostatistics/kxv026},
  DOI = {10.1093/biostatistics/kxv026},
  number = {1},
  journal = {Biostatistics},
  publisher = {Oxford University Press (OUP)},
  author = {Jacob,  Laurent and Gagnon-Bartsch,  Johann A. and Speed,  Terence P.},
  year = {2015},
  month = aug,
  pages = {16–28}
}

@article{Gihawi2023,
  title = {Major data analysis errors invalidate cancer microbiome findings},
  volume = {14},
  ISSN = {2150-7511},
  url = {http://dx.doi.org/10.1128/mbio.01607-23},
  DOI = {10.1128/mbio.01607-23},
  number = {5},
  journal = {mBio},
  publisher = {American Society for Microbiology},
  author = {Gihawi,  Abraham and Ge,  Yuchen and Lu,  Jennifer and Puiu,  Daniela and Xu,  Amanda and Cooper,  Colin S. and Brewer,  Daniel S. and Pertea,  Mihaela and Salzberg,  Steven L.},
  editor = {Zhulin,  Igor B.},
  year = {2023},
  month = oct 
}

@article{latour1987science,
  title={Science in action: How to follow scientists and engineers through society},
  author={Latour, Bruno},
  journal={Harvard UP},
  year={1987}
}

@article{sarkar2021separating,
  title={Separating measurement and expression models clarifies confusion in single-cell RNA sequencing analysis},
  author={Sarkar, Abhishek and Stephens, Matthew},
  journal={Nature genetics},
  volume={53},
  number={6},
  pages={770--777},
  year={2021},
  publisher={Nature Publishing Group US New York}
}

@article{jiang2022statistics,
  title={Statistics or biology: the zero-inflation controversy about scRNA-seq data},
  author={Jiang, Ruochen and Sun, Tianyi and Song, Dongyuan and Li, Jingyi Jessica},
  journal={Genome biology},
  volume={23},
  number={1},
  pages={31},
  year={2022},
  publisher={Springer}
}

@article{Gihawi2023,
  title = {Caution Regarding the Specificities of Pan-Cancer Microbial Structure},
  url = {http://dx.doi.org/10.1101/2023.01.16.523562},
  DOI = {10.1101/2023.01.16.523562},
  publisher = {Cold Spring Harbor Laboratory},
  author = {Gihawi,  Abraham and Cooper,  Colin S. and Brewer,  Daniel S.},
  year = {2023},
  month = jan 
}

@article{SepichPoore2023,
  title = {Reply to: Caution Regarding the Specificities of Pan-Cancer Microbial Structure},
  url = {http://dx.doi.org/10.1101/2023.02.10.528049},
  DOI = {10.1101/2023.02.10.528049},
  publisher = {Cold Spring Harbor Laboratory},
  author = {Sepich-Poore,  Gregory D. and Kopylova,  Evguenia and Zhu,  Qiyun and Carpenter,  Carolina and Fraraccio,  Serena and Wandro,  Stephen and Kosciolek,  Tomasz and Janssen,  Stefan and Metcalf,  Jessica and Song,  Se Jin and Kanbar,  Jad and Miller-Montgomery,  Sandrine and Heaton,  Robert and Mckay,  Rana and Patel,  Sandip Pravin and Swafford,  Austin D and Knight,  Rob},
  year = {2023},
  month = feb 
}

@article{SepichPoore2024,
  title = {Robustness of cancer microbiome signals over a broad range of methodological variation},
  volume = {43},
  ISSN = {1476-5594},
  url = {http://dx.doi.org/10.1038/s41388-024-02974-w},
  DOI = {10.1038/s41388-024-02974-w},
  number = {15},
  journal = {Oncogene},
  publisher = {Springer Science and Business Media LLC},
  author = {Sepich-Poore,  Gregory D. and McDonald,  Daniel and Kopylova,  Evguenia and Guccione,  Caitlin and Zhu,  Qiyun and Austin,  George and Carpenter,  Carolina and Fraraccio,  Serena and Wandro,  Stephen and Kosciolek,  Tomasz and Janssen,  Stefan and Metcalf,  Jessica L. and Song,  Se Jin and Kanbar,  Jad and Miller-Montgomery,  Sandrine and Heaton,  Robert and Mckay,  Rana and Patel,  Sandip Pravin and Swafford,  Austin D. and Korem,  Tal and Knight,  Rob},
  year = {2024},
  month = feb,
  pages = {1127–1148}
}

@article{narunsky2022pan,
  title={Pan-cancer analyses reveal cancer-type-specific fungal ecologies and bacteriome interactions},
  author={Narunsky-Haziza, Lian and Sepich-Poore, Gregory D and Livyatan, Ilana and Asraf, Omer and Martino, Cameron and Nejman, Deborah and Gavert, Nancy and Stajich, Jason E and Amit, Guy and Gonz{\'a}lez, Antonio and others},
  journal={Cell},
  volume={185},
  number={20},
  pages={3789--3806},
  year={2022},
  publisher={Elsevier}
}

@article{mecham2010supervised,
  title={Supervised normalization of microarrays},
  author={Mecham, Brigham H and Nelson, Peter S and Storey, John D},
  journal={Bioinformatics},
  volume={26},
  number={10},
  pages={1308--1315},
  year={2010},
  publisher={Oxford University Press}
}

@InProceedings{pmlr-v119-koh20a,
  title = 	 {Concept Bottleneck Models},
  author =       {Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5338--5348},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/koh20a/koh20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/koh20a.html},
  abstract = 	 {We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art models today do not typically support the manipulation of concepts like "the existence of bone spurs", as they are trained end-to-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts ("bone spurs") or bird attributes ("wing color"). These models also allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time.}
}

@article{Bengio2009,
  title = {Learning Deep Architectures for AI},
  volume = {2},
  ISSN = {1935-8245},
  url = {http://dx.doi.org/10.1561/2200000006},
  DOI = {10.1561/2200000006},
  number = {1},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  publisher = {Now Publishers},
  author = {Bengio,  Y.},
  year = {2009},
  pages = {1–127}
}

@misc{
alain2017understanding,
title={Understanding intermediate layers using linear classifier probes},
author={Guillaume Alain and Yoshua Bengio},
year={2017},
url={https://openreview.net/forum?id=ryF7rTqgl}
}

@article{templeton2024scaling,
    title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
    author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
    year={2024},
    journal={Transformer Circuits Thread},
    url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}

@ARTICLE{Friedman2001-xk,
  title     = "Greedy function approximation: A gradient boosting machine",
  author    = "Friedman, Jerome H",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  29,
  number    =  5,
  pages     = "1189--1232",
  month     =  oct,
  year      =  2001
}

@ARTICLE{Kim2017-kb,
  title        = "Interpretability beyond feature attribution: Quantitative
                  testing with Concept Activation Vectors ({TCAV})",
  author       = "Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai,
                  Carrie and Wexler, James and Viegas, Fernanda and Sayres,
                  Rory",
  abstract     = "The interpretation of deep learning models is a challenge due
                  to their size, complexity, and often opaque internal state.
                  In addition, many systems, such as image classifiers, operate
                  on low-level features rather than high-level concepts. To
                  address these challenges, we introduce Concept Activation
                  Vectors (CAVs), which provide an interpretation of a neural
                  net's internal state in terms of human-friendly concepts. The
                  key idea is to view the high-dimensional internal state of a
                  neural net as an aid, not an obstacle. We show how to use
                  CAVs as part of a technique, Testing with CAVs (TCAV), that
                  uses directional derivatives to quantify the degree to which
                  a user-defined concept is important to a classification
                  result--for example, how sensitive a prediction of ``zebra''
                  is to the presence of stripes. Using the domain of image
                  classification as a testing ground, we describe how CAVs may
                  be used to explore hypotheses and generate insights for a
                  standard image classification network as well as a medical
                  application.",
  journal      = "arXiv [stat.ML]",
  publisher    = "arXiv",
  year         =  2017,
  primaryClass = "stat.ML"
}

@INPROCEEDINGS{Yun2021-jc,
  title      = "Transformer visualization via dictionary learning:
                contextualized embedding as a linear superposition of
                transformer factors",
  booktitle  = "Proceedings of Deep Learning Inside Out ({DeeLIO)}: The 2nd
                Workshop on Knowledge Extraction and Integration for Deep
                Learning Architectures",
  author     = "Yun, Zeyu and Chen, Yubei and Olshausen, Bruno and LeCun, Yann",
  publisher  = "Association for Computational Linguistics",
  year       =  2021,
  address    = "Stroudsburg, PA, USA",
  conference = "Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd
                Workshop on Knowledge Extraction and Integration for Deep
                Learning Architectures",
  location   = "Online"
}

@ARTICLE{Apley2020-nd,
  title     = "Visualizing the effects of predictor variables in black box
               supervised learning models",
  author    = "Apley, Daniel W and Zhu, Jingyu",
  abstract  = "Summary In many supervised learning applications, understanding
               and visualizing the effects of the predictor variables on the
               predicted response is of paramount importance. A shortcoming of
               black box supervised learning models (e.g. complex trees, neural
               networks, boosted trees, random forests, nearest neighbours,
               local kernel-weighted methods and support vector regression) in
               this regard is their lack of interpretability or transparency.
               Partial dependence plots, which are the most popular approach
               for visualizing the effects of the predictors with black box
               supervised learning models, can produce erroneous results if the
               predictors are strongly correlated, because they require
               extrapolation of the response at predictor values that are far
               outside the multivariate envelope of the training data. As an
               alternative to partial dependence plots, we present a new
               visualization approach that we term accumulated local effects
               plots, which do not require this unreliable extrapolation with
               correlated predictors. Moreover, accumulated local effects plots
               are far less computationally expensive than partial dependence
               plots. We also provide an R package ALEPlot as supplementary
               material to implement our proposed method.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Oxford University Press (OUP)",
  volume    =  82,
  number    =  4,
  pages     = "1059--1086",
  month     =  sep,
  year      =  2020,
  copyright = "https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model",
  language  = "en"
}

@ARTICLE{Agarwal2023-sx,
  title        = "{MDI+}: A flexible random forest-based feature importance
                  framework",
  author       = "Agarwal, Abhineet and Kenney, Ana M and Tan, Yan Shuo and
                  Tang, Tiffany M and Yu, Bin",
  abstract     = "Mean decrease in impurity (MDI) is a popular feature
                  importance measure for random forests (RFs). We show that the
                  MDI for a feature $X_k$ in each tree in an RF is equivalent
                  to the unnormalized $R^2$ value in a linear regression of the
                  response on the collection of decision stumps that split on
                  $X_k$. We use this interpretation to propose a flexible
                  feature importance framework called MDI+. Specifically, MDI+
                  generalizes MDI by allowing the analyst to replace the linear
                  regression model and $R^2$ metric with regularized
                  generalized linear models (GLMs) and metrics better suited
                  for the given data structure. Moreover, MDI+ incorporates
                  additional features to mitigate known biases of decision
                  trees against additive or smooth models. We further provide
                  guidance on how practitioners can choose an appropriate GLM
                  and metric based upon the Predictability, Computability,
                  Stability framework for veridical data science. Extensive
                  data-inspired simulations show that MDI+ significantly
                  outperforms popular feature importance measures in
                  identifying signal features. We also apply MDI+ to two
                  real-world case studies on drug response prediction and
                  breast cancer subtype classification. We show that MDI+
                  extracts well-established predictive genes with significantly
                  greater stability compared to existing feature importance
                  measures. All code and models are released in a full-fledged
                  python package on Github.",
  year         =  2023,
  primaryClass = "stat.ME",
  eprint       = "2307.01932"
}

@INPROCEEDINGS{Ribeiro2016-qk,
  title           = "Why should {I} trust you?",
  booktitle       = "Proceedings of the 22nd {ACM} {SIGKDD} International
                     Conference on Knowledge Discovery and Data Mining",
  author          = "Ribeiro, Marco Tulio and Singh, Sameer and Guestrin,
                     Carlos",
  publisher       = "ACM",
  month           =  aug,
  year            =  2016,
  address         = "New York, NY, USA",
  copyright       = "http://www.acm.org/publications/policies/copyright\_policy\#Background",
  conference      = "KDD '16: The 22nd ACM SIGKDD International Conference on
                     Knowledge Discovery and Data Mining",
  location        = "San Francisco California USA"
}

@misc{stoehr2024,
  doi = {10.48550/ARXIV.2403.19851},
  url = {https://arxiv.org/abs/2403.19851},
  author = {Stoehr,  Niklas and Gordon,  Mitchell and Zhang,  Chiyuan and Lewis,  Owen},
  keywords = {Computation and Language (cs.CL),  Cryptography and Security (cs.CR),  Machine Learning (cs.LG),  Machine Learning (stat.ML),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Localizing Paragraph Memorization in Language Models},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{karpathy2015,
  doi = {10.48550/ARXIV.1506.02078},
  url = {https://arxiv.org/abs/1506.02078},
  author = {Karpathy,  Andrej and Johnson,  Justin and Fei-Fei,  Li},
  keywords = {Machine Learning (cs.LG),  Computation and Language (cs.CL),  Neural and Evolutionary Computing (cs.NE),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Visualizing and Understanding Recurrent Networks},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@article{Lucieri2022,
  title = {ExAID: A multimodal explanation framework for computer-aided diagnosis of skin lesions},
  volume = {215},
  ISSN = {0169-2607},
  url = {http://dx.doi.org/10.1016/j.cmpb.2022.106620},
  DOI = {10.1016/j.cmpb.2022.106620},
  journal = {Computer Methods and Programs in Biomedicine},
  publisher = {Elsevier BV},
  author = {Lucieri,  Adriano and Bajwa,  Muhammad Naseer and Braun,  Stephan Alexander and Malik,  Muhammad Imran and Dengel,  Andreas and Ahmed,  Sheraz},
  year = {2022},
  month = mar,
  pages = {106620}
}

@inproceedings{atanasova-etal-2020-diagnostic,
    title = "A Diagnostic Study of Explainability Techniques for Text Classification",
    author = "Atanasova, Pepa  and
      Simonsen, Jakob Grue  and
      Lioma, Christina  and
      Augenstein, Isabelle",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.263",
    doi = "10.18653/v1/2020.emnlp-main.263",
    pages = "3256--3274",
    abstract = "Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models{'} predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model{'}s performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.",
}

@Manual{lime_package,
  title = {lime: Local Interpretable Model-Agnostic Explanations},
  author = {Emil Hvitfeldt and Thomas Lin Pedersen and Michaël Benesty},
  year = {2022},
  note = {https://lime.data-imaginist.com, https://github.com/thomasp85/lime},
}

% Other papers to include
% the recent concept papers - are they reliable?
% some of the evaluation papers?
% Some packages. You can have a pointer to relevant packages at the very end
% your own paper!!
% 

@inproceedings{
freeman2024exploring,
title={Exploring Memorization and Copyright Violation in Frontier {LLM}s: A Study of the New York Times v. Open{AI} 2023 Lawsuit},
author={Joshua Freeman and Chloe Rippe and Edoardo Debenedetti and Maksym Andriushchenko},
booktitle={Neurips Safe Generative AI Workshop 2024},
year={2024},
url={https://openreview.net/forum?id=C66DBl9At8}
}

@misc{nyt_case,
  title        = {The New York Times Company Lawsuit Document 1-68},
  number       = {1:23-cv-11195},
  court        = {U.S. District Court},
  year         = {2023},
  type         = {Court Filing},
  note         = {Case No. 1:23-cv-11195, Document 1-68}
}

@inbook{Zeiler2014,
  title = {Visualizing and Understanding Convolutional Networks},
  ISBN = {9783319105901},
  ISSN = {1611-3349},
  url = {http://dx.doi.org/10.1007/978-3-319-10590-1_53},
  DOI = {10.1007/978-3-319-10590-1_53},
  booktitle = {Computer Vision – ECCV 2014},
  publisher = {Springer International Publishing},
  author = {Zeiler,  Matthew D. and Fergus,  Rob},
  year = {2014},
  pages = {818–833}
}

@article{Caruana2015IntelligibleMF,
  title={Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission},
  author={Rich Caruana and Yin Lou and Johannes Gehrke and Paul Koch and M. Sturm and No{\'e}mie Elhadad},
  journal={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year={2015},
}


@article{kundaliya2023,
author={Kundaliya,Dev},
year={2023},
month={Feb 09},
title={Computing - Incisive Media: Google AI chatbot Bard gives wrong answer in its first demo},
journal={Computing},
keywords={Computers; Accuracy; Solar system; Space telescopes; Artificial intelligence; Chatbots},
isbn={13612972},
language={English},
url={https://ezproxy.library.wisc.edu/login?url=https://www.proquest.com/trade-journals/computing-incisive-media-google-ai-chatbot-bard/docview/2774438186/se-2},
}

@ARTICLE{Sankaran2024-ny,
  title     = "Data Science Principles for Interpretable and Explainable {AI}",
  author    = "Sankaran, Kris",
  abstract  = "Society's capacity for algorithmic problem-solving has never
               been greater. Artificial Intelligence is now applied across more
               domains than ever, a consequence of powerful abstractions,
               abundant data, and accessible software. As capabilities have
               expanded, so have risks, with models often deployed without
               fully understanding their potential impacts. Interpretable and
               interactive machine learning aims to make complex models more
               transparent and controllable, enhancing user agency. This review
               synthesizes key principles from the growing literature in this
               field. We first introduce precise vocabulary for discussing
               interpretability, like the distinction between glass box and
               explainable algorithms. We then explore connections to classical
               statistical and design principles, like parsimony and the gulfs
               of interaction. Basic explainability techniques -- including
               learned embeddings, integrated gradients, and concept
               bottlenecks -- are illustrated with a simple case study. We also
               review criteria for objectively evaluating interpretability
               approaches. Throughout, we underscore the importance of
               considering audience goals when designing interactive
               algorithmic systems. Finally, we outline open challenges and
               discuss the potential role of data science in addressing them.
               Code to reproduce all examples can be found at
               https://go.wisc.edu/3k1ewe.",
  journal = "arXiv",
  year      =  2024
}

@article{kundaliya2023,
author={Kundaliya,Dev},
year={2023},
month={Feb 09},
title={Computing - Incisive Media: Google AI chatbot Bard gives wrong answer in its first demo},
journal={Computing},
keywords={Computers; Accuracy; Solar system; Space telescopes; Artificial intelligence; Chatbots},
isbn={13612972},
language={English},
url={https://ezproxy.library.wisc.edu/login?url=https://www.proquest.com/trade-journals/computing-incisive-media-google-ai-chatbot-bard/docview/2774438186/se-2},
}

@article{Gu2019,
  title = {BadNets: Evaluating Backdooring Attacks on Deep Neural Networks},
  volume = {7},
  ISSN = {2169-3536},
  url = {http://dx.doi.org/10.1109/ACCESS.2019.2909068},
  DOI = {10.1109/access.2019.2909068},
  journal = {IEEE Access},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author = {Gu,  Tianyu and Liu,  Kang and Dolan-Gavitt,  Brendan and Garg,  Siddharth},
  year = {2019},
  pages = {47230–47244}
}

@inproceedings{50351,title	= {Concept Bottleneck Models},author	= {Pang Wei Koh and Thao Nguyen and Yew Siang Tang and Stephen Mussmann and Emma Pierson and Been Kim and Percy Liang},year	= {2020}}


@inproceedings{
yuksekgonul2023posthoc,
title={Post-hoc Concept Bottleneck Models},
author={Mert Yuksekgonul and Maggie Wang and James Zou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=nA5AZ8CEyow}
}

@inbook{10.5555/3454287.3455119,
author = {Ghorbani, Amirata and Wexler, James and Zou, James and Kim, Been},
title = {Towards automatic concept-based explanations},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for concept based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that ACE discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {832},
numpages = {10}
}

@misc{overlooked_factors,
  doi = {10.48550/ARXIV.2207.09615},
  url = {https://arxiv.org/abs/2207.09615},
  author = {Ramaswamy,  Vikram V. and Kim,  Sunnie S. Y. and Fong,  Ruth and Russakovsky,  Olga},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Overlooked factors in concept-based explanations: Dataset choice,  concept learnability,  and human capability},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@misc{medium_beyond_nodate,
	title = {Beyond {Interpretability}: {Developing} a {Language} to {Shape} {Our} {Relationships} with {AI}},
	shorttitle = {Beyond {Interpretability}},
	url = {https://cacmb4.acm.org/opinion/articles/260816-beyond-interpretability-developing-a-language-to-shape-our-relationships-with-ai/fulltext},
	abstract = {Wouldn\&\#39;t it be nice if we could ask AI questions to learn how and why it makes its predictions?},
	language = {en},
  year={2022},
	urldate = {2024-05-16},
	author = {Been Kim},
  journal = {Communications of the ACM},
	file = {Snapshot:/Users/krissankaran/Zotero/storage/WT4FPMM7/fulltext.html:text/html},
}

@InProceedings{tcav,
  title = 	 {Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors ({TCAV})},
  author =       {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and sayres, Rory},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2668--2677},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kim18d/kim18d.pdf},
  url = 	 {https://proceedings.mlr.press/v80/kim18d.html},
  abstract = 	 {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result–for example, how sensitive a prediction of “zebra” is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.}
}

@article{Zhang2024,
  title = {Recovery of biological signals lost in single-cell batch integration with CellANOVA},
  ISSN = {1546-1696},
  url = {http://dx.doi.org/10.1038/s41587-024-02463-1},
  DOI = {10.1038/s41587-024-02463-1},
  journal = {Nature Biotechnology},
  publisher = {Springer Science and Business Media LLC},
  author = {Zhang,  Zhaojun and Mathew,  Divij and Lim,  Tristan L. and Mason,  Kaishu and Martinez,  Clara Morral and Huang,  Sijia and Wherry,  E. John and Susztak,  Katalin and Minn,  Andy J. and Ma,  Zongming and Zhang,  Nancy R.},
  year = {2024},
  month = nov 
}