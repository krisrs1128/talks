
@InProceedings{pmlr-v119-koh20a,
  title = 	 {Concept Bottleneck Models},
  author =       {Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5338--5348},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/koh20a/koh20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/koh20a.html},
  abstract = 	 {We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art models today do not typically support the manipulation of concepts like "the existence of bone spurs", as they are trained end-to-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts ("bone spurs") or bird attributes ("wing color"). These models also allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time.}
}

@article{Bengio2009,
  title = {Learning Deep Architectures for AI},
  volume = {2},
  ISSN = {1935-8245},
  url = {http://dx.doi.org/10.1561/2200000006},
  DOI = {10.1561/2200000006},
  number = {1},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  publisher = {Now Publishers},
  author = {Bengio,  Y.},
  year = {2009},
  pages = {1–127}
}

@misc{
alain2017understanding,
title={Understanding intermediate layers using linear classifier probes},
author={Guillaume Alain and Yoshua Bengio},
year={2017},
url={https://openreview.net/forum?id=ryF7rTqgl}
}

@article{templeton2024scaling,
    title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
    author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
    year={2024},
    journal={Transformer Circuits Thread},
    url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}

@ARTICLE{Friedman2001-xk,
  title     = "Greedy function approximation: A gradient boosting machine",
  author    = "Friedman, Jerome H",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  29,
  number    =  5,
  pages     = "1189--1232",
  month     =  oct,
  year      =  2001
}

@ARTICLE{Kim2017-kb,
  title        = "Interpretability beyond feature attribution: Quantitative
                  testing with Concept Activation Vectors ({TCAV})",
  author       = "Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai,
                  Carrie and Wexler, James and Viegas, Fernanda and Sayres,
                  Rory",
  abstract     = "The interpretation of deep learning models is a challenge due
                  to their size, complexity, and often opaque internal state.
                  In addition, many systems, such as image classifiers, operate
                  on low-level features rather than high-level concepts. To
                  address these challenges, we introduce Concept Activation
                  Vectors (CAVs), which provide an interpretation of a neural
                  net's internal state in terms of human-friendly concepts. The
                  key idea is to view the high-dimensional internal state of a
                  neural net as an aid, not an obstacle. We show how to use
                  CAVs as part of a technique, Testing with CAVs (TCAV), that
                  uses directional derivatives to quantify the degree to which
                  a user-defined concept is important to a classification
                  result--for example, how sensitive a prediction of ``zebra''
                  is to the presence of stripes. Using the domain of image
                  classification as a testing ground, we describe how CAVs may
                  be used to explore hypotheses and generate insights for a
                  standard image classification network as well as a medical
                  application.",
  journal      = "arXiv [stat.ML]",
  publisher    = "arXiv",
  year         =  2017,
  primaryClass = "stat.ML"
}

@INPROCEEDINGS{Yun2021-jc,
  title      = "Transformer visualization via dictionary learning:
                contextualized embedding as a linear superposition of
                transformer factors",
  booktitle  = "Proceedings of Deep Learning Inside Out ({DeeLIO)}: The 2nd
                Workshop on Knowledge Extraction and Integration for Deep
                Learning Architectures",
  author     = "Yun, Zeyu and Chen, Yubei and Olshausen, Bruno and LeCun, Yann",
  publisher  = "Association for Computational Linguistics",
  year       =  2021,
  address    = "Stroudsburg, PA, USA",
  conference = "Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd
                Workshop on Knowledge Extraction and Integration for Deep
                Learning Architectures",
  location   = "Online"
}

@ARTICLE{Apley2020-nd,
  title     = "Visualizing the effects of predictor variables in black box
               supervised learning models",
  author    = "Apley, Daniel W and Zhu, Jingyu",
  abstract  = "Summary In many supervised learning applications, understanding
               and visualizing the effects of the predictor variables on the
               predicted response is of paramount importance. A shortcoming of
               black box supervised learning models (e.g. complex trees, neural
               networks, boosted trees, random forests, nearest neighbours,
               local kernel-weighted methods and support vector regression) in
               this regard is their lack of interpretability or transparency.
               Partial dependence plots, which are the most popular approach
               for visualizing the effects of the predictors with black box
               supervised learning models, can produce erroneous results if the
               predictors are strongly correlated, because they require
               extrapolation of the response at predictor values that are far
               outside the multivariate envelope of the training data. As an
               alternative to partial dependence plots, we present a new
               visualization approach that we term accumulated local effects
               plots, which do not require this unreliable extrapolation with
               correlated predictors. Moreover, accumulated local effects plots
               are far less computationally expensive than partial dependence
               plots. We also provide an R package ALEPlot as supplementary
               material to implement our proposed method.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Oxford University Press (OUP)",
  volume    =  82,
  number    =  4,
  pages     = "1059--1086",
  month     =  sep,
  year      =  2020,
  copyright = "https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model",
  language  = "en"
}

@ARTICLE{Agarwal2023-sx,
  title        = "{MDI+}: A flexible random forest-based feature importance
                  framework",
  author       = "Agarwal, Abhineet and Kenney, Ana M and Tan, Yan Shuo and
                  Tang, Tiffany M and Yu, Bin",
  abstract     = "Mean decrease in impurity (MDI) is a popular feature
                  importance measure for random forests (RFs). We show that the
                  MDI for a feature $X_k$ in each tree in an RF is equivalent
                  to the unnormalized $R^2$ value in a linear regression of the
                  response on the collection of decision stumps that split on
                  $X_k$. We use this interpretation to propose a flexible
                  feature importance framework called MDI+. Specifically, MDI+
                  generalizes MDI by allowing the analyst to replace the linear
                  regression model and $R^2$ metric with regularized
                  generalized linear models (GLMs) and metrics better suited
                  for the given data structure. Moreover, MDI+ incorporates
                  additional features to mitigate known biases of decision
                  trees against additive or smooth models. We further provide
                  guidance on how practitioners can choose an appropriate GLM
                  and metric based upon the Predictability, Computability,
                  Stability framework for veridical data science. Extensive
                  data-inspired simulations show that MDI+ significantly
                  outperforms popular feature importance measures in
                  identifying signal features. We also apply MDI+ to two
                  real-world case studies on drug response prediction and
                  breast cancer subtype classification. We show that MDI+
                  extracts well-established predictive genes with significantly
                  greater stability compared to existing feature importance
                  measures. All code and models are released in a full-fledged
                  python package on Github.",
  year         =  2023,
  primaryClass = "stat.ME",
  eprint       = "2307.01932"
}

@INPROCEEDINGS{Ribeiro2016-qk,
  title           = "Why should {I} trust you?",
  booktitle       = "Proceedings of the 22nd {ACM} {SIGKDD} International
                     Conference on Knowledge Discovery and Data Mining",
  author          = "Ribeiro, Marco Tulio and Singh, Sameer and Guestrin,
                     Carlos",
  publisher       = "ACM",
  month           =  aug,
  year            =  2016,
  address         = "New York, NY, USA",
  copyright       = "http://www.acm.org/publications/policies/copyright\_policy\#Background",
  conference      = "KDD '16: The 22nd ACM SIGKDD International Conference on
                     Knowledge Discovery and Data Mining",
  location        = "San Francisco California USA"
}

@misc{stoehr2024,
  doi = {10.48550/ARXIV.2403.19851},
  url = {https://arxiv.org/abs/2403.19851},
  author = {Stoehr,  Niklas and Gordon,  Mitchell and Zhang,  Chiyuan and Lewis,  Owen},
  keywords = {Computation and Language (cs.CL),  Cryptography and Security (cs.CR),  Machine Learning (cs.LG),  Machine Learning (stat.ML),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Localizing Paragraph Memorization in Language Models},
  publisher = {arXiv},
  year = {2024},
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{karpathy2015,
  doi = {10.48550/ARXIV.1506.02078},
  url = {https://arxiv.org/abs/1506.02078},
  author = {Karpathy,  Andrej and Johnson,  Justin and Fei-Fei,  Li},
  keywords = {Machine Learning (cs.LG),  Computation and Language (cs.CL),  Neural and Evolutionary Computing (cs.NE),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Visualizing and Understanding Recurrent Networks},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@article{Lucieri2022,
  title = {ExAID: A multimodal explanation framework for computer-aided diagnosis of skin lesions},
  volume = {215},
  ISSN = {0169-2607},
  url = {http://dx.doi.org/10.1016/j.cmpb.2022.106620},
  DOI = {10.1016/j.cmpb.2022.106620},
  journal = {Computer Methods and Programs in Biomedicine},
  publisher = {Elsevier BV},
  author = {Lucieri,  Adriano and Bajwa,  Muhammad Naseer and Braun,  Stephan Alexander and Malik,  Muhammad Imran and Dengel,  Andreas and Ahmed,  Sheraz},
  year = {2022},
  month = mar,
  pages = {106620}
}

@inproceedings{atanasova-etal-2020-diagnostic,
    title = "A Diagnostic Study of Explainability Techniques for Text Classification",
    author = "Atanasova, Pepa  and
      Simonsen, Jakob Grue  and
      Lioma, Christina  and
      Augenstein, Isabelle",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.263",
    doi = "10.18653/v1/2020.emnlp-main.263",
    pages = "3256--3274",
    abstract = "Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models{'} predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model{'}s performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.",
}

@Manual{lime_package,
  title = {lime: Local Interpretable Model-Agnostic Explanations},
  author = {Emil Hvitfeldt and Thomas Lin Pedersen and Michaël Benesty},
  year = {2022},
  note = {https://lime.data-imaginist.com, https://github.com/thomasp85/lime},
}

% Other papers to include
% the recent concept papers - are they reliable?
% some of the evaluation papers?
% Some packages. You can have a pointer to relevant packages at the very end
% your own paper!!
%

@inproceedings{
freeman2024exploring,
title={Exploring Memorization and Copyright Violation in Frontier {LLM}s: A Study of the New York Times v. Open{AI} 2023 Lawsuit},
author={Joshua Freeman and Chloe Rippe and Edoardo Debenedetti and Maksym Andriushchenko},
booktitle={Neurips Safe Generative AI Workshop 2024},
year={2024},
url={https://openreview.net/forum?id=C66DBl9At8}
}

@misc{nyt_case,
  title        = {The New York Times Company Lawsuit Document 1-68},
  number       = {1:23-cv-11195},
  court        = {U.S. District Court},
  year         = {2023},
  type         = {Court Filing},
  note         = {Case No. 1:23-cv-11195, Document 1-68}
}

@inbook{Zeiler2014,
  title = {Visualizing and Understanding Convolutional Networks},
  ISBN = {9783319105901},
  ISSN = {1611-3349},
  url = {http://dx.doi.org/10.1007/978-3-319-10590-1_53},
  DOI = {10.1007/978-3-319-10590-1_53},
  booktitle = {Computer Vision – ECCV 2014},
  publisher = {Springer International Publishing},
  author = {Zeiler,  Matthew D. and Fergus,  Rob},
  year = {2014},
  pages = {818–833}
}

@article{Caruana2015IntelligibleMF,
  title={Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission},
  author={Rich Caruana and Yin Lou and Johannes Gehrke and Paul Koch and M. Sturm and No{\'e}mie Elhadad},
  journal={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year={2015},
}


@article{kundaliya2023,
author={Kundaliya,Dev},
year={2023},
month={Feb 09},
title={Computing - Incisive Media: Google AI chatbot Bard gives wrong answer in its first demo},
journal={Computing},
keywords={Computers; Accuracy; Solar system; Space telescopes; Artificial intelligence; Chatbots},
isbn={13612972},
language={English},
url={https://ezproxy.library.wisc.edu/login?url=https://www.proquest.com/trade-journals/computing-incisive-media-google-ai-chatbot-bard/docview/2774438186/se-2},
}

@ARTICLE{Sankaran2024-ny,
  title     = "Data Science Principles for Interpretable and Explainable {AI}",
  author    = "Sankaran, Kris",
  abstract  = "Society's capacity for algorithmic problem-solving has never
               been greater. Artificial Intelligence is now applied across more
               domains than ever, a consequence of powerful abstractions,
               abundant data, and accessible software. As capabilities have
               expanded, so have risks, with models often deployed without
               fully understanding their potential impacts. Interpretable and
               interactive machine learning aims to make complex models more
               transparent and controllable, enhancing user agency. This review
               synthesizes key principles from the growing literature in this
               field. We first introduce precise vocabulary for discussing
               interpretability, like the distinction between glass box and
               explainable algorithms. We then explore connections to classical
               statistical and design principles, like parsimony and the gulfs
               of interaction. Basic explainability techniques -- including
               learned embeddings, integrated gradients, and concept
               bottlenecks -- are illustrated with a simple case study. We also
               review criteria for objectively evaluating interpretability
               approaches. Throughout, we underscore the importance of
               considering audience goals when designing interactive
               algorithmic systems. Finally, we outline open challenges and
               discuss the potential role of data science in addressing them.
               Code to reproduce all examples can be found at
               https://go.wisc.edu/3k1ewe.",
  journal = "arXiv",
  year      =  2024
}

@article{kundaliya2023,
author={Kundaliya,Dev},
year={2023},
month={Feb 09},
title={Computing - Incisive Media: Google AI chatbot Bard gives wrong answer in its first demo},
journal={Computing},
keywords={Computers; Accuracy; Solar system; Space telescopes; Artificial intelligence; Chatbots},
isbn={13612972},
language={English},
url={https://ezproxy.library.wisc.edu/login?url=https://www.proquest.com/trade-journals/computing-incisive-media-google-ai-chatbot-bard/docview/2774438186/se-2},
}

@article{Gu2019,
  title = {BadNets: Evaluating Backdooring Attacks on Deep Neural Networks},
  volume = {7},
  ISSN = {2169-3536},
  url = {http://dx.doi.org/10.1109/ACCESS.2019.2909068},
  DOI = {10.1109/access.2019.2909068},
  journal = {IEEE Access},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author = {Gu,  Tianyu and Liu,  Kang and Dolan-Gavitt,  Brendan and Garg,  Siddharth},
  year = {2019},
  pages = {47230–47244}
}

@inproceedings{50351,title	= {Concept Bottleneck Models},author	= {Pang Wei Koh and Thao Nguyen and Yew Siang Tang and Stephen Mussmann and Emma Pierson and Been Kim and Percy Liang},year	= {2020}}


@inproceedings{
yuksekgonul2023posthoc,
title={Post-hoc Concept Bottleneck Models},
author={Mert Yuksekgonul and Maggie Wang and James Zou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=nA5AZ8CEyow}
}

@inbook{10.5555/3454287.3455119,
author = {Ghorbani, Amirata and Wexler, James and Zou, James and Kim, Been},
title = {Towards automatic concept-based explanations},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for concept based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that ACE discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {832},
numpages = {10}
}

@misc{overlooked_factors,
  doi = {10.48550/ARXIV.2207.09615},
  url = {https://arxiv.org/abs/2207.09615},
  author = {Ramaswamy,  Vikram V. and Kim,  Sunnie S. Y. and Fong,  Ruth and Russakovsky,  Olga},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Overlooked factors in concept-based explanations: Dataset choice,  concept learnability,  and human capability},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}

@misc{medium_beyond_nodate,
	title = {Beyond {Interpretability}: {Developing} a {Language} to {Shape} {Our} {Relationships} with {AI}},
	shorttitle = {Beyond {Interpretability}},
	url = {https://cacmb4.acm.org/opinion/articles/260816-beyond-interpretability-developing-a-language-to-shape-our-relationships-with-ai/fulltext},
	abstract = {Wouldn\&\#39;t it be nice if we could ask AI questions to learn how and why it makes its predictions?},
	language = {en},
  year={2022},
	urldate = {2024-05-16},
	author = {Been Kim},
  journal = {Communications of the ACM},
	file = {Snapshot:/Users/krissankaran/Zotero/storage/WT4FPMM7/fulltext.html:text/html},
}

@InProceedings{tcav,
  title = 	 {Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors ({TCAV})},
  author =       {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and sayres, Rory},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2668--2677},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kim18d/kim18d.pdf},
  url = 	 {https://proceedings.mlr.press/v80/kim18d.html},
  abstract = 	 {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net’s internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result–for example, how sensitive a prediction of “zebra” is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.}
}
