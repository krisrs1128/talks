Our society’s capacity for algorithmic problem solving has perhaps never been
higher — the combination of powerful abstractions, plentiful data, and
accessible software means that machine learning is now a central tool across
various industries and areas of research. At the same time, we are regularly
warned of the risks of misusing this capacity (e.g., the New York Times in 2024:
“Imran Khan’s ‘Victory Speech’ From Jail Shows A.I.’s Peril and Promise,” “The
Trouble With A.I. Sharpening,” “Are A.I. Mammograms Worth Their Cost?”).
Interpretable and interactive machine learning aims to make complex models
understandable and controllable, enhancing user agency and creativity. This
course will review principles from the growing literature on this topic. We will
introduce precise vocabulary for discussing interpretability, for example, the
distinction between glassbox and explainable algorithms. Connections to
classical statistical and design principles, like parsimony and the gulfs of
interaction, will be highlighted. We will review the mechanics of basic
explainability techniques, including feature embeddings, integrated gradients,
and concept bottlenecks. Real-world applications and simplified code demos will
illustrate how interpretability can support progress on longstanding challenges,
like out-of-domain generalization and fairness. Criteria for objectively
evaluating interpretability mechanisms will be discussed. The role of the
audience’s perspective interactive algorithm design will be emphasized. Finally,
we will explore open problems and the potential role for statistical thinking in
addressing them.
 
Target Audience
A basic course in applied statistics and statistical machine learning will be
assumed (for example, dimensionality reduction, sparse regression, and CART).
Prior experience with R and python will be helpful for following code demos.